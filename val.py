# -*- coding: utf-8 -*-
import os
import sys
import argparse
import torch
from tqdm import tqdm

# ===================== è·¯å¾„ä¿®å¤ =====================
# æŠŠ uie å­ç›®å½•åŠ å…¥ Python è·¯å¾„ï¼Œç¡®ä¿èƒ½æ‰¾åˆ° datasets ç­‰æ¨¡å—
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, os.path.join(SCRIPT_DIR, 'uie'))

# ===================== å¯¼å…¥æ¨¡å— =====================
# æ³¨æ„ï¼šdataset.py é‡Œçš„ç±»åæ˜¯ ImageDatasetï¼Œä¸æ˜¯ FlowerDataset
from datasets.dataset import ImageDataset
from datasets.transforms import get_val_transforms
from models.vit import create_vit_model
from utils.config import load_config

def main():
    parser = argparse.ArgumentParser(description="æ‰¹é‡éªŒè¯è„šæœ¬ï¼ˆé€‚é…é¡¹ç›®ç»“æ„ï¼‰")
    parser.add_argument("--checkpoint", type=str, default=None, 
                        help="æ¨¡å‹æƒé‡è·¯å¾„ï¼Œé»˜è®¤åŠ è½½ work_dirs/best.pt")
    args = parser.parse_args()

    # ===================== 1. åŠ è½½é…ç½® =====================
    # ä½ çš„ load_config ä¸éœ€è¦ä¼ å‚ï¼Œç›´æ¥è°ƒç”¨
    cfg = load_config()
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    num_classes = cfg["data"]["num_classes"]
    class_names = ["daisy", "dandelion", "rose", "sunflower", "tulip"]  # 5ç±»èŠ±æœµåç§°

    # ===================== 2. æ„å»ºæ¨¡å‹ & åŠ è½½æƒé‡ =====================
    model = create_vit_model(num_classes=num_classes).to(device)
    
    if args.checkpoint:
        checkpoint_path = args.checkpoint
    else:
        checkpoint_path = os.path.join(cfg["train"]["save_path"], "best.pt")

    if not os.path.exists(checkpoint_path):
        raise FileNotFoundError(f"æƒé‡æ–‡ä»¶æœªæ‰¾åˆ°: {checkpoint_path}")

    model.load_state_dict(torch.load(checkpoint_path, map_location=device))
    model.eval()
    print(f"âœ… æˆåŠŸåŠ è½½æƒé‡: {checkpoint_path}")

    # ===================== 3. æ„å»ºéªŒè¯é›† DataLoader =====================
    val_transform = get_val_transforms(cfg["data"]["image_size"])
    val_dataset = ImageDataset(
        data_root=cfg["data"]["val_root"],
        transforms=val_transform
    )
    val_loader = torch.utils.data.DataLoader(
        val_dataset,
        batch_size=cfg["train"]["batch_size"],
        shuffle=False,
        num_workers=cfg["train"]["num_workers"]
    )

    # ===================== 4. æ‰¹é‡æ¨ç† & è®¡ç®—å‡†ç¡®ç‡ =====================
    total_correct = 0
    total_num = 0

    with torch.no_grad():
        for imgs, labels in tqdm(val_loader, desc="ğŸ” æ­£åœ¨éªŒè¯"):
            imgs = imgs.to(device)
            labels = labels.to(device)

            outputs = model(imgs)
            preds = torch.argmax(outputs, dim=1)

            total_correct += (preds == labels).sum().item()
            total_num += imgs.size(0)

    val_acc = total_correct / total_num
    print(f"\n=== éªŒè¯ç»“æœ ===")
    print(f"ğŸ“Š æ€»æ ·æœ¬æ•°: {total_num}")
    print(f"ğŸ¯ æ•´ä½“å‡†ç¡®ç‡: {val_acc:.4f} ({val_acc*100:.2f}%)")

if __name__ == "__main__":
    main()